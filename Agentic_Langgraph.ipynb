{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec125a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, TypedDict, List, Dict, Any, Optional\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "import uuid\n",
    "from dotenv import load_dotenv\n",
    "from langchain.agents import Tool\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258e727a",
   "metadata": {},
   "source": [
    "### Load environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65685991",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec0f8ff",
   "metadata": {},
   "source": [
    "### Tool 1: Search tool using Google Serper API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ad0e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "serper = GoogleSerperAPIWrapper()\n",
    "#serper.run(\"What is the capital of India?\")\n",
    "tool_search =Tool(\n",
    "        name=\"search\",\n",
    "        func=serper.run,\n",
    "        description=\"Useful for when you need more information from an online search\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b228389",
   "metadata": {},
   "source": [
    "### Tool 2: push notification tool using Pushover API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f733a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure to have the Pushover API token and user key set in environment variables\n",
    "pushover_token = os.getenv(\"PUSHOVER_TOKEN\")\n",
    "pushover_user = os.getenv(\"PUSHOVER_USER\")\n",
    "pushover_url = \"https://api.pushover.net/1/messages.json\"\n",
    "\n",
    "def push(text: str):\n",
    "    \"\"\"Send a push notification to the user\"\"\"\n",
    "    requests.post(pushover_url, data = {\"token\": pushover_token, \"user\": pushover_user, \"message\": text})\n",
    "\n",
    "tool_push = Tool(\n",
    "        name=\"send_push_notification\",\n",
    "        func=push,\n",
    "        description=\"useful for when you want to send a push notification\"\n",
    "    )\n",
    "\n",
    "#tool_push.invoke(\"Hello, me\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d69f43c",
   "metadata": {},
   "source": [
    "### Define the tools to be used in the graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bf4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools= [tool_search, tool_push]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a831b6",
   "metadata": {},
   "source": [
    "### Define a structured output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a4faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EvaluatorOutput(BaseModel):\n",
    "    feedback: str = Field(description=\"Feedback on the assistant's response\")\n",
    "    success_criteria_met: bool = Field(description=\"Whether the success criteria have been met\")\n",
    "    user_input_needed: bool = Field(description=\"True if more input is needed from the user, or clarifications, or the assistant is stuck\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f73ad3",
   "metadata": {},
   "source": [
    "### Define the state of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will include the messages, success criteria, feedback, and flags for user input and success\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[Any], add_messages]\n",
    "    success_criteria: str\n",
    "    feedback_on_work: Optional[str]\n",
    "    success_criteria_met: bool\n",
    "    user_input_needed: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd7223",
   "metadata": {},
   "source": [
    "### Create a ChatOpenAI instance with the models for Worker/Assistant and Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a574a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ChatOpenAI instance with the model you want to use and bind the tools to it\n",
    "# This worker/assistant LLM will be used to generate the response and can call the tools\n",
    "# The tools will be used to search for information or send notifications as needed\n",
    "\n",
    "worker_llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "worker_llm_with_tools = worker_llm.bind_tools(tools)\n",
    "\n",
    "# Initialize the evaluator LLM with structured output\n",
    "# This LLM will be used to evaluate the assistant's response based on the success criteria\n",
    "\n",
    "evaluator_llm = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "evaluator_llm_with_output = evaluator_llm.with_structured_output(EvaluatorOutput)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844792fc",
   "metadata": {},
   "source": [
    "### The Worker/Assistant node function \n",
    "\n",
    "This function prepares a system prompt for the assistant (including task criteria and feedback), ensures it’s included in the message history, invokes the LLM with tools, and returns the updated state with the assistant’s response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Defines a function worker that takes a state dictionary and returns a dictionary.\n",
    "def worker(state: State) -> Dict[str, Any]:\n",
    "    #Creates a system_message string that instructs the assistant on how to behave, including the task’s success criteria from the state. \n",
    "    system_message = f\"\"\"You are a helpful assistant that can use tools to complete tasks.\n",
    "You keep working on a task until either you have a question or clarification for the user, or the success criteria is met.\n",
    "This is the success criteria:\n",
    "{state['success_criteria']}\n",
    "You should reply either with a question for the user about this assignment, or with your final response.\n",
    "If you have a question for the user, you need to reply by clearly stating your question. An example might be:\n",
    "\n",
    "Question: please clarify whether you want a summary or a detailed answer\n",
    "\n",
    "If you've finished, reply with the final answer, and don't ask a question; simply reply with the answer.\n",
    "\"\"\"\n",
    "    # If there is feedback on previous work, include it in the system message to guide the assistant and \n",
    "    # help it meet the success criteria.\n",
    "    if state.get(\"feedback_on_work\"):\n",
    "        system_message += f\"\"\"\n",
    "Previously you thought you completed the assignment, but your reply was rejected because the success criteria was not met.\n",
    "Here is the feedback on why this was rejected:\n",
    "{state['feedback_on_work']}\n",
    "With this feedback, please continue the assignment, ensuring that you meet the success criteria or have a question for the user.\"\"\"\n",
    "    \n",
    "    # Checks if there is already a SystemMessage in the messages list.\n",
    "    # If found, updates its content with the new system_message.\n",
    "    # Add in the system message\n",
    "\n",
    "    found_system_message = False\n",
    "    messages = state[\"messages\"]\n",
    "    for message in messages:\n",
    "        if isinstance(message, SystemMessage):\n",
    "            message.content = system_message\n",
    "            found_system_message = True\n",
    "    # If no SystemMessage was found, prepend the system message to the messages list.\n",
    "    # This ensures that the system message is always present at the start of the conversation.\n",
    "    if not found_system_message:\n",
    "        messages = [SystemMessage(content=system_message)] + messages\n",
    "    \n",
    "    # Invoke the LLM with tools to generate a response based on the current messages.\n",
    "    response = worker_llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Return updated state\n",
    "    # The response is added to the messages list, and the state is updated with the new messages.\n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895bba30",
   "metadata": {},
   "source": [
    "### Worker Router Function\n",
    "\n",
    "This function decides the next step in a workflow based on the last message in the state. It acts as a router. If the last message requests tool usage, it returns \"tools\". Otherwise, it returns \"evaluator\". This helps the workflow decide what to do next based on the assistant’s output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc741c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_router(state: State) -> str:\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    \n",
    "    if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    else:\n",
    "        return \"evaluator\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a012c50",
   "metadata": {},
   "source": [
    "### Converts a list of user and assistant messages into a readable chat transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a93951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling each line as either \"User\" or \"Assistant\" and handling cases where the assistant’s message is empty \n",
    "# (e.g., when a tool was used).\n",
    "def format_conversation(messages: List[Any]) -> str:\n",
    "    conversation = \"Conversation history:\\n\\n\"\n",
    "    for message in messages:\n",
    "        if isinstance(message, HumanMessage):\n",
    "            conversation += f\"User: {message.content}\\n\"\n",
    "        elif isinstance(message, AIMessage):\n",
    "            text = message.content or \"[Tools use]\"\n",
    "            conversation += f\"Assistant: {text}\\n\"\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497f4ab2",
   "metadata": {},
   "source": [
    "### Evaluator function\n",
    "\n",
    "This function uses an LLM to evaluate the assistant’s last answer against the assignment’s success criteria, provides feedback, and decides if the task is complete or if more user input is needed. It updates the workflow state with this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cbd2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluator(state: State) -> State:\n",
    "    last_response = state[\"messages\"][-1].content\n",
    "    \n",
    "    # Prepares a system prompt for the evaluator LLM, instructing it to judge the assistant’s answer against the success criteria.\n",
    "    \n",
    "    system_message = f\"\"\"You are an evaluator that determines if a task has been completed successfully by an Assistant.\n",
    "Assess the Assistant's last response based on the given criteria. Respond with your feedback, and with your decision on whether the success criteria has been met,\n",
    "and whether more input is needed from the user.\"\"\"\n",
    "    \n",
    "    # Builds a detailed user prompt for the evaluator LLM, including: \n",
    "    # The full conversation history (formatted),\n",
    "    # The assignment’s success criteria,\n",
    "    # The assistant’s last response,\n",
    "    # Instructions to give feedback and decide if the criteria are met or if more user input is needed.\n",
    "    \n",
    "    user_message = f\"\"\"You are evaluating a conversation between the User and Assistant. You decide what action to take based on the last response from the Assistant.\n",
    "\n",
    "The entire conversation with the assistant, with the user's original request and all replies, is:\n",
    "{format_conversation(state['messages'])}\n",
    "\n",
    "The success criteria for this assignment is:\n",
    "{state['success_criteria']}\n",
    "\n",
    "And the final response from the Assistant that you are evaluating is:\n",
    "{last_response}\n",
    "\n",
    "Respond with your feedback, and decide if the success criteria is met by this response.\n",
    "Also, decide if more user input is required, either because the assistant has a question, needs clarification, or seems to be stuck and unable to answer without help.\n",
    "\"\"\"\n",
    "    # If there was previous feedback, appends it to the prompt, reminding the evaluator to check for repeated mistakes.\n",
    "    if state[\"feedback_on_work\"]:\n",
    "        user_message += f\"Also, note that in a prior attempt from the Assistant, you provided this feedback: {state['feedback_on_work']}\\n\"\n",
    "        user_message += \"If you're seeing the Assistant repeating the same mistakes, then consider responding that user input is required.\"\n",
    "    \n",
    "    # Prepares the message list for the LLM: a system message (instructions) and a human message (the evaluation task).\n",
    "    evaluator_messages = [SystemMessage(content=system_message), HumanMessage(content=user_message)]\n",
    "    \n",
    "    # Calls the evaluator LLM with the prepared messages and gets the result (which should include feedback, a success flag, and a user input flag).\n",
    "    eval_result = evaluator_llm_with_output.invoke(evaluator_messages)\n",
    "    \n",
    "    # Builds a new state dictionary: \n",
    "    # Adds the evaluator’s feedback as a message, Stores the feedback, whether the criteria were met, \n",
    "    # and if more user input is needed.\n",
    "    new_state = {\n",
    "        \"messages\": [{\"role\": \"assistant\", \"content\": f\"Evaluator Feedback on this answer: {eval_result.feedback}\"}],\n",
    "        \"feedback_on_work\": eval_result.feedback,\n",
    "        \"success_criteria_met\": eval_result.success_criteria_met,\n",
    "        \"user_input_needed\": eval_result.user_input_needed\n",
    "    }\n",
    "    return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cd2b6",
   "metadata": {},
   "source": [
    "### Router function routes the workflow from Evaluator\n",
    "\n",
    "If the task is done or needs user input, it ends the workflow.\n",
    "Otherwise, it sends the state back to the worker for more work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce37e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_based_on_evaluation(state: State) -> str:\n",
    "    \n",
    "    # If the task’s success criteria have been met (success_criteria_met is True), \n",
    "    # or if more input is needed from the user (user_input_needed is True), \n",
    "    # the function returns \"END\", signaling the workflow to stop or move to the end node.\n",
    "    if state[\"success_criteria_met\"] or state[\"user_input_needed\"]:\n",
    "        return \"END\"\n",
    "    else:\n",
    "        return \"worker\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7516051b",
   "metadata": {},
   "source": [
    "### Build a LangGraph workflow \n",
    "\n",
    "Three main nodes (`worker`, `tools`, `evaluator`),\n",
    "sets up the logic for moving between them based on the state, and \n",
    "compiles the workflow for execution with memory checkpointing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b21b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Graph Builder with State\n",
    "# Create a StateGraph instance to manage the workflow\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "# Adds three nodes to the graph: \n",
    "#   worker: Runs the `worker` function (handles main task logic). \n",
    "#   tools: Runs a `ToolNode` (handles tool calls, e.g., web search, calculator).\n",
    "#   evaluator: Runs the `evaluator` function (checks if the task is complete or needs user input).\n",
    "graph_builder.add_node(\"worker\", worker)\n",
    "graph_builder.add_node(\"tools\", ToolNode(tools=tools))\n",
    "graph_builder.add_node(\"evaluator\", evaluator)\n",
    "\n",
    "# Add edges\n",
    "# Adds conditional edges from the worker node: \n",
    "# Uses the worker_router function to decide the next node.\n",
    "# If worker_router returns tools, go to the tools node.\n",
    "# If it returns evaluator, go to the evaluator node.\n",
    "graph_builder.add_conditional_edges(\"worker\", worker_router, {\"tools\": \"tools\", \"evaluator\": \"evaluator\"})\n",
    "graph_builder.add_edge(\"tools\", \"worker\")\n",
    "graph_builder.add_conditional_edges(\"evaluator\", route_based_on_evaluation, {\"worker\": \"worker\", \"END\": END})\n",
    "graph_builder.add_edge(START, \"worker\")\n",
    "\n",
    "# Compile the graph\n",
    "# Compiles the graph into an executable workflow.\n",
    "# Uses MemorySaver() as a checkpointer to save the state/progress of the workflow.\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38646858",
   "metadata": {},
   "source": [
    "### Generates and displays a visual diagram of LangGraph workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c77776",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8783f51a",
   "metadata": {},
   "source": [
    "#### Let's Test the workflow :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb116e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    \"messages\": [HumanMessage(content=\"What is the GDP of USA?\")],\n",
    "    \"success_criteria\": \"The answer must include the latest GDP figure for the USA.\",\n",
    "    \"feedback_on_work\": \"\",\n",
    "    \"success_criteria_met\": False,\n",
    "    \"user_input_needed\": False\n",
    "}\n",
    "result = graph.invoke(\n",
    "    state,\n",
    "    config={\"thread_id\": \"my-unique-thread-id\"}\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8425bf",
   "metadata": {},
   "source": [
    "### Print the messages in the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfe349",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will display the conversation history, including the user's input, assistant's responses, and any tool calls made.\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4441c8",
   "metadata": {},
   "source": [
    "### Next comes the gradio Callback to kick off a super-step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b9eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# make_thread_id() function generates a unique thread ID as a string using Python’s uuid module.\n",
    "#  It is useful for tracking or checkpointing conversations or sessions uniquely.\n",
    "\n",
    "def make_thread_id() -> str:\n",
    "    return str(uuid.uuid4())\n",
    "\n",
    "def format_flow(messages):\n",
    "    # Build a string in your desired format\n",
    "    flow = \"\"\n",
    "    for m in messages:\n",
    "        if isinstance(m, HumanMessage):\n",
    "            flow += \"======== Human Message ========\\n\" + m.content + \"\\n\"\n",
    "        elif hasattr(m, \"tool_calls\") and m.tool_calls:\n",
    "            flow += \"======== Ai Message (Tool Calls) ========\\n\"\n",
    "            for call in m.tool_calls:\n",
    "                flow += f\"Tool: {call['name']}\\nArgs: {call['args']}\\n\"\n",
    "        elif isinstance(m, AIMessage):\n",
    "            flow += \"======== Ai Message ========\\n\" + m.content + \"\\n\"\n",
    "        elif isinstance(m, dict) and m.get(\"role\") == \"assistant\":\n",
    "            flow += \"======== Ai Message ========\\n\" + m.get(\"content\", \"\") + \"\\n\"\n",
    "        else:\n",
    "            flow += str(m) + \"\\n\"\n",
    "    return flow\n",
    "\n",
    "\n",
    "def format_for_gradio(messages):\n",
    "    formatted = []\n",
    "    for m in messages:\n",
    "        # Skip evaluator feedback\n",
    "        if (\n",
    "            isinstance(m, dict)\n",
    "            and m.get(\"role\") == \"assistant\"\n",
    "            and isinstance(m.get(\"content\"), str)\n",
    "            and m[\"content\"].startswith(\"Evaluator Feedback on this answer:\")\n",
    "        ):\n",
    "            continue  # Skip this message\n",
    "\n",
    "        # If m is a dict with 'role' and 'content'\n",
    "        if isinstance(m, dict) and \"role\" in m and \"content\" in m:\n",
    "            formatted.append([m[\"role\"], m[\"content\"]])\n",
    "        # If m is a LangChain message object\n",
    "        elif hasattr(m, \"content\"):\n",
    "            role = getattr(m, \"role\", m.__class__.__name__.lower())\n",
    "            formatted.append([role, m.content])\n",
    "        else:\n",
    "            formatted.append([\"assistant\", str(m)])\n",
    "    return formatted\n",
    "\n",
    "# process_message function handles a single interaction in the conversation workflow, updating the state and \n",
    "# returning the updated conversation history.\n",
    "# Parameters:\n",
    "# message: The current user message(s) (could be a list or a single message).\n",
    "# success_criteria: The criteria for a successful answer.\n",
    "# history: The conversation history so far.\n",
    "# thread: The unique thread ID for this conversation.\n",
    "\n",
    "async def process_message(message, success_criteria, history, thread):\n",
    "\n",
    "    config = {\"configurable\": {\"thread_id\": thread}}\n",
    "\n",
    "    state = {\n",
    "        \"messages\": message,\n",
    "        \"success_criteria\": success_criteria,\n",
    "        \"feedback_on_work\": None,\n",
    "        \"success_criteria_met\": False,\n",
    "        \"user_input_needed\": False\n",
    "    }\n",
    "    result = await graph.ainvoke(state, config=config)\n",
    "    user = {\"role\": \"user\", \"content\": message}\n",
    "    reply = {\"role\": \"assistant\", \"content\": result[\"messages\"][-2].content}\n",
    "    feedback = {\"role\": \"assistant\", \"content\": result[\"messages\"][-1].content}\n",
    "    \n",
    "    return format_for_gradio(history + [user, reply, feedback]), format_flow(history + [user, reply, feedback])\n",
    "    #return format_for_gradio(history + [user, reply, feedback])\n",
    "    #return history + [user, reply, feedback]\n",
    "\n",
    "# Resets the conversation state.\n",
    "# Returns: Empty values for message, success_criteria, history, and a new unique thread ID.\n",
    "async def reset():\n",
    "    return \"\", \"\", None, make_thread_id()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948cf06c",
   "metadata": {},
   "source": [
    "### And now launch our UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168bee5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks(theme=gr.themes.Default(primary_hue=\"emerald\")) as demo:\n",
    "    gr.Markdown(\"## Personal Co-worker\")\n",
    "    thread = gr.State(make_thread_id())\n",
    "    \n",
    "    with gr.Row():\n",
    "        chatbot = gr.Chatbot(label=\"How Can I Help You today ? \", height=300)\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            message = gr.Textbox(show_label=False, placeholder=\"Your request to your sidekick\")\n",
    "        with gr.Row():\n",
    "            success_criteria = gr.Textbox(show_label=False, placeholder=\"What are your success critiera?\")\n",
    "    with gr.Row():\n",
    "        reset_button = gr.Button(\"Reset\", variant=\"stop\")\n",
    "        go_button = gr.Button(\"Go!\", variant=\"primary\")\n",
    "    with gr.Row():\n",
    "        flow_window = gr.Code(label=\"Conversation Flow\", language=\"python\", interactive=False, lines=20)\n",
    "        \n",
    "    #message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    #success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    #go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot])\n",
    "    #reset_button.click(reset, [], [message, success_criteria, chatbot, thread])\n",
    "    message.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot, flow_window])\n",
    "    success_criteria.submit(process_message, [message, success_criteria, chatbot, thread], [chatbot, flow_window])\n",
    "    go_button.click(process_message, [message, success_criteria, chatbot, thread], [chatbot, flow_window])\n",
    "    reset_button.click(reset, [], [message, success_criteria, chatbot, thread, flow_window])\n",
    "    \n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb90048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will display the conversation history, including the user's input, assistant's responses, and any tool calls made.\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "print(gr.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
